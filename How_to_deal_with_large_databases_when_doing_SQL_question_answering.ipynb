{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to deal with large databases when doing SQL question-answering\n",
        "\n",
        "When using LangChain to ask a language model to write SQL queries, the model needs to know your database's structure. This means providing table names, how the tables are organized (schemas), and the types of data stored in the columns.\n",
        "\n",
        "However, real-world databases can be huge. Directly copying all that information into every prompt is impractical. Instead, we need a smart way to give the model only the essential details it needs for the specific query.\n",
        "\n",
        "This guide shows you how to use LangChain to:\n",
        "\n",
        "Figure out which tables are actually relevant to the user's question. We don't want to overwhelm the model with information about tables it doesn't need.\n",
        "\n",
        "Identify the specific values within those relevant columns that the model should use in its query. This helps the model focus on the data that matters.\n",
        "\n",
        "In essence, we'll learn how to make LangChain intelligently select and provide only the necessary database information to the language model, so it can generate accurate and efficient SQL queries."
      ],
      "metadata": {
        "id": "9VXIa3bifsrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install necessary dependencies"
      ],
      "metadata": {
        "id": "Us6wDK28htQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install sqlite3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esFKDfJS1mAd",
        "outputId": "72e19296-2568-4652-fb61-329d20b0e6ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,704 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,639 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,950 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,319 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,664 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,813 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,532 kB]\n",
            "Fetched 28.9 MB in 4s (7,186 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  sqlite3-doc\n",
            "The following NEW packages will be installed:\n",
            "  sqlite3\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 768 kB of archives.\n",
            "After this operation, 1,873 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.3 [768 kB]\n",
            "Fetched 768 kB in 2s (419 kB/s)\n",
            "Selecting previously unselected package sqlite3.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Setting up sqlite3 (3.37.2-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-experimental langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d7zMbl-aYqC",
        "outputId": "2066377f-588e-4e89-fd75-83eeb8346333"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setup Colab environment\n",
        "\n",
        "This code snippet sets up an environment in Google Colab to create and populate an SQLite database called Chinook.db. It does this by:\n",
        "\n",
        "Mounting your Google Drive. Creating a specific directory in your Google Drive to store the database. Downloading a SQL script from a GitHub repository. Using the sqlite3 command-line tool to create the database and execute the SQL script, thereby populating the database with data. This is a typical workflow for setting up a data science environment in Google Colab, where you need to access and create files in your Google Drive and use command-line tools to interact with data."
      ],
      "metadata": {
        "id": "nwxKDKGsejgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounts Google Drive\n",
        "\n",
        "a. from google.colab import drive: Imports the drive module from the google.colab library, which is specific to Google Colaboratory.\n",
        "\n",
        "b. drive.mount('/content/drive'): This line mounts your Google Drive to the Colab runtime. After executing this, you'll be prompted to authorize Colab to access your Google Drive. Once authorized, your Drive files become accessible within the Colab environment under the /content/drive directory."
      ],
      "metadata": {
        "id": "AB2r4fxme6XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSYTyP8-ZqHy",
        "outputId": "38603aa3-d1a5-49e6-e956-197a172d7539"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sets up the directory\n",
        "\n",
        "a. import os: Imports the os module for interacting with the operating system, specifically for file and directory operations.\n",
        "\n",
        "b. directory_path = '/content/drive/MyDrive/Colab Notebooks/Chinook': Defines the path to a directory within your Google Drive. This is where the SQLite database will be created.\n",
        "\n",
        "c. if not os.path.exists(directory_path): os.makedirs(directory_path): This checks if the specified directory exists. If it doesn't, it creates the directory and any necessary parent directories. This is important to ensure that the database file can be created in the correct location."
      ],
      "metadata": {
        "id": "MtudPa1BfEHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the directory within your Google Drive\n",
        "directory_path = '/content/drive/MyDrive/Colab Notebooks/Chinook'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)"
      ],
      "metadata": {
        "id": "Cj5Hkl1cZzWi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloads and Creates the SQLite Database\n",
        "\n",
        "This line executes a shell command that downloads a SQL script and uses it to create an SQLite database."
      ],
      "metadata": {
        "id": "Py6uYeO9fRvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 '/content/drive/MyDrive/Colab Notebooks/Chinook/Chinook.db'"
      ],
      "metadata": {
        "id": "qNDAmaFKaA2J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the connection\n",
        "\n",
        "This code snippet establishes a connection to an SQLite database (Chinook.db) using LangChain's SQLDatabase utility, prints the database dialect and usable table names, and then executes a simple SQL query to retrieve and display the first 20 rows from the Artist table. This demonstrates how to use LangChain to interact with SQL databases and execute SQL queries."
      ],
      "metadata": {
        "id": "OcrYy82jfZyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:////content/drive/MyDrive/Colab Notebooks/Chinook/Chinook.db\", sample_rows_in_table_info=3)\n",
        "print(db.dialect)\n",
        "print(db.get_usable_table_names())\n",
        "print(db.run(\"SELECT * FROM Artist LIMIT 10;\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEE721u2aGfC",
        "outputId": "de5b690c-dfaf-4c57-f10c-a324b35a4972"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sqlite\n",
            "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n",
            "[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Handling Many tables\n",
        "\n",
        "When using LangChain to generate SQL queries, we need to tell the language model about the structure of our database tables (schemas). If we have a lot of tables, it's impossible to fit all that information into a single prompt. Instead, we need a smarter way to provide only the relevant table schemas.\n",
        "\n",
        "LangChain provides a powerful way to do this using \"tool-calling.\" This allows us to ask the language model to first identify the tables that are actually needed for the user's question, and then only provide the schemas of those specific tables.\n",
        "\n",
        "Here's how we do it:\n",
        "\n",
        "We use LangChain's .bind_tools method to tell the language model that it has access to a tool that can provide a list of relevant table names. This tool is defined using Pydantic, which ensures the output is in a predictable format.\n",
        "\n",
        "We use an output parser to take the language model's response (which should be a list of table names) and turn it into a usable object within LangChain.\n",
        "\n",
        "In simpler terms, we're teaching LangChain to ask the language model: \"Which tables do I need to know about?\" Then, LangChain only provides the schemas of those specific tables, making the process more efficient and accurate."
      ],
      "metadata": {
        "id": "pUtMrPcEvW-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select chat model"
      ],
      "metadata": {
        "id": "XavHBautv93l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "id": "TZRYoPFApXVf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "Fm6EVX4NwDyu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain, Pydantic, and OpenAI tools to identify relevant SQL tables for a given user question\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. Table Pydantic Model\n",
        "\n",
        "Table(BaseModel): Defines a Pydantic model called Table.\n",
        "\n",
        "name: str = Field(...): Specifies that the model has a single field named name, which is a string.\n",
        "\n",
        "description: Provides a description of the field, which is used by the LLM to understand the purpose of the tool.\n",
        "\n",
        "3. Preparing Table Names\n",
        "\n",
        "db.get_usable_table_names(): This assumes that db is a database object (similar to previous examples) and that it has a method called get_usable_table_names(). This method is expected to return a list of table names that are available for querying.\n",
        "\n",
        "\"\\n\".join(...): This joins the list of table names into a single string, with each table name separated by a newline character.\n",
        "\n",
        "4. Creating System Message\n",
        "\n",
        "This creates a system message that will be used to instruct the LLM.\n",
        "\n",
        "It tells the LLM to identify all potentially relevant tables for a user question.\n",
        "\n",
        "It provides the list of available table names.\n",
        "\n",
        "It emphasizes that the LLM should include all potentially relevant tables, even if it's unsure.\n",
        "\n",
        "5. Creating Prompt Template\n",
        "\n",
        "ChatPromptTemplate.from_messages(...): Creates a chat prompt template with a system message and a human message.\n",
        "\n",
        "(\"system\", system): Adds the system message created earlier.\n",
        "\n",
        "(\"human\", \"{input}\"): Adds a human message with a placeholder for the user's input.\n",
        "\n",
        "6. Binding Tools and Creating Output Parser\n",
        "\n",
        "llm_with_tools = llm.bind_tools([Table]): Binds the Table Pydantic model to the LLM as a tool. This allows the LLM to output a Table object.\n",
        "\n",
        "output_parser = PydanticToolsParser(tools=[Table]): Creates an output parser that can parse the LLM's output into a Table object.\n",
        "\n",
        "7. Creating the Chain\n",
        "\n",
        "Creates a LangChain runnable chain:\n",
        "\n",
        "prompt: The prompt template.\n",
        "\n",
        "llm_with_tools: The LLM with the Table tool bound.\n",
        "\n",
        "output_parser: The output parser.\n",
        "\n",
        "8. Invoking the Chain\n",
        "\n",
        "table_chain.invoke(...): Invokes the chain with the user's question.\n",
        "\n",
        "{\"input\": \"What are all the genres of Alanis Morisette songs\"}: Provides the user's question as input.\n",
        "\n"
      ],
      "metadata": {
        "id": "5w1S8akRwn5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Table(BaseModel):\n",
        "    \"\"\"Table in SQL database.\"\"\"\n",
        "\n",
        "    name: str = Field(description=\"Name of table in SQL database.\")\n",
        "\n",
        "\n",
        "table_names = \"\\n\".join(db.get_usable_table_names())\n",
        "system = f\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \\\n",
        "The tables are:\n",
        "\n",
        "{table_names}\n",
        "\n",
        "Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "llm_with_tools = llm.bind_tools([Table])\n",
        "output_parser = PydanticToolsParser(tools=[Table])\n",
        "\n",
        "table_chain = prompt | llm_with_tools | output_parser\n",
        "\n",
        "table_chain.invoke({\"input\": \"What are all the genres of Alanis Morisette songs\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XPvbxP0wG9F",
        "outputId": "5ce27017-74fb-43f0-e508-1f5b9134dd9a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='Artist'),\n",
              " Table(name='Genre'),\n",
              " Table(name='Album'),\n",
              " Table(name='Track')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it Works (Simplified):\n",
        "\n",
        "Prompt Generation: The prompt template is formatted with the system message and the user's question.\n",
        "LLM Invocation: The formatted prompt is sent to the LLM.\n",
        "Tool Usage: The LLM uses the Table tool (which expects a table name) to identify and return relevant table categories.\n",
        "Output Parsing: The PydanticToolsParser parses the LLM's output into a list of Table objects.\n",
        "Result: The chain returns a list of Table objects, where each object's name attribute represents a table category that the LLM believes is relevant to the user's question.\n",
        "\n",
        "summary\n",
        "\n",
        "This code snippet demonstrates how to use an LLM to identify relevant table categories for a user's question. It simplifies the previous example by using a predefined list of table categories and a simpler system message. This can be useful for routing questions to different parts of an application based on the identified categories."
      ],
      "metadata": {
        "id": "EikP9TS75zdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"Return the names of any SQL tables that are relevant to the user question.\n",
        "The tables are:\n",
        "\n",
        "Music\n",
        "Business\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "category_chain = prompt | llm_with_tools | output_parser\n",
        "category_chain.invoke({\"input\": \"What are all the genres of Alanis Morisette songs\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw0XeWxWxy6K",
        "outputId": "ef55f638-66ab-4e24-939f-81731c1824cd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='Music')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain chain to identify relevant SQL tables based on the identified categories\n",
        "\n",
        "1. get_tables Function\n",
        "\n",
        "Purpose: This function takes a list of Table objects (from the previous explanations) as input, where each Table object represents a category. It then returns a list of actual table names based on the categories.\n",
        "\n",
        "Logic:\n",
        "\n",
        "It initializes an empty list called tables.\n",
        "\n",
        "It iterates through the categories list.\n",
        "\n",
        "For each category:\n",
        "\n",
        "If the category.name is \"Music\", it extends the tables list with the names of tables related to music (e.g., \"Album\", \"Artist\", \"Genre\", etc.).\n",
        "\n",
        "If the category.name is \"Business\", it extends the tables list with the names of tables related to business (e.g., \"Customer\", \"Employee\", \"Invoice\", etc.).\n",
        "\n",
        "Finally, it returns the tables list.\n",
        "\n",
        "2. Creating the Chain\n",
        "\n",
        "category_chain: This is assumed to be the LangChain chain created in the previous explanation, which identifies the relevant categories based on the user's question.\n",
        "\n",
        "|: This is the LangChain \"pipe\" operator, which allows you to chain together LangChain runnables.\n",
        "\n",
        "get_tables: This is the custom function defined earlier.\n",
        "\n",
        "This line creates a new LangChain chain table_chain by chaining category_chain and get_tables. This means that the output of category_chain (a list of Table objects) will be passed as input to the get_tables function.\n",
        "\n",
        "3. Invoking the Chain\n",
        "\n",
        "Summary\n",
        "\n",
        "This code demonstrates how to use a LangChain chain to identify relevant SQL tables based on the identified categories from a previous step. It chains together a category identification chain and a custom function to generate a list of table names. This allows you to dynamically determine the relevant tables for a user's question, which can be useful for building more flexible and intelligent applications."
      ],
      "metadata": {
        "id": "eKzykZXx6rdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def get_tables(categories: List[Table]) -> List[str]:\n",
        "    tables = []\n",
        "    for category in categories:\n",
        "        if category.name == \"Music\":\n",
        "            tables.extend(\n",
        "                [\n",
        "                    \"Album\",\n",
        "                    \"Artist\",\n",
        "                    \"Genre\",\n",
        "                    \"MediaType\",\n",
        "                    \"Playlist\",\n",
        "                    \"PlaylistTrack\",\n",
        "                    \"Track\",\n",
        "                ]\n",
        "            )\n",
        "        elif category.name == \"Business\":\n",
        "            tables.extend([\"Customer\", \"Employee\", \"Invoice\", \"InvoiceLine\"])\n",
        "    return tables\n",
        "\n",
        "\n",
        "table_chain = category_chain | get_tables\n",
        "table_chain.invoke({\"input\": \"What are all the genres of Alanis Morisette songs\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRZVK-9kyFF5",
        "outputId": "5f5917e5-cf43-4f5b-ac4d-72335dd15662"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Album', 'Artist', 'Genre', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the LangChain chain\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. query_chain Creation\n",
        "\n",
        "This line creates a query_chain using the create_sql_query_chain function.\n",
        "\n",
        "llm: Assumed to be an initialized language model.\n",
        "\n",
        "db: Assumed to be an initialized database connection object.\n",
        "\n",
        "This chain will take a natural language question and generate an SQL query, execute it against the database, and return the results.\n",
        "\n",
        "3. table_chain Modification\n",
        "\n",
        "table_chain: This is assumed to be the chain created in the previous explanation, which identifies relevant tables based on the user's question.\n",
        "\n",
        "{\"input\": itemgetter(\"question\")}: This creates a dictionary that maps the \"question\" key in the input to the \"input\" key expected by the table_chain.\n",
        "\n",
        "itemgetter(\"question\"): This creates a callable object that retrieves the value associated with the \"question\" key from the input dictionary.\n",
        "\n",
        "|: This is the LangChain \"pipe\" operator, which chains together runnables.\n",
        "\n",
        "This line modifies the table_chain so that it expects the input question to be under the \"question\" key instead of the \"input\" key.\n",
        "\n",
        "4. full_chain Creation\n",
        "\n",
        "RunnablePassthrough.assign(...): This creates a RunnablePassthrough object that assigns the output of a runnable to a key in the input dictionary.\n",
        "\n",
        "table_names_to_use=table_chain: This specifies that the output of the table_chain (a list of table names) should be assigned to the \"table_names_to_use\" key in the input dictionary.\n",
        "\n",
        "| query_chain: This chains the RunnablePassthrough with the query_chain.\n",
        "\n",
        "Summary\n",
        "\n",
        "This code creates a LangChain chain that dynamically selects the relevant tables to query based on the user's question, and then generates and executes an SQL query against those tables. This allows the system to handle questions that involve different tables or categories of data."
      ],
      "metadata": {
        "id": "ufrmOJrg7l0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "query_chain = create_sql_query_chain(llm, db)\n",
        "# Convert \"question\" key to the \"input\" key expected by current table_chain.\n",
        "table_chain = {\"input\": itemgetter(\"question\")} | table_chain\n",
        "# Set table_names_to_use using table_chain.\n",
        "full_chain = RunnablePassthrough.assign(table_names_to_use=table_chain) | query_chain"
      ],
      "metadata": {
        "id": "dE1XaphjyOBq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = full_chain.invoke(\n",
        "    {\"question\": \"What are all the genres of Alanis Morisette songs\"}\n",
        ")\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH2_rjY4yZpH",
        "outputId": "740d5b73-635b-4589-b1b7-98a543ba8bcf"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQLQuery: \n",
            "```sql\n",
            "SELECT DISTINCT \"Genre\".\"Name\"\n",
            "FROM \"Track\"\n",
            "JOIN \"Album\" ON \"Track\".\"AlbumId\" = \"Album\".\"AlbumId\"\n",
            "JOIN \"Artist\" ON \"Album\".\"ArtistId\" = \"Artist\".\"ArtistId\"\n",
            "JOIN \"Genre\" ON \"Track\".\"GenreId\" = \"Genre\".\"GenreId\"\n",
            "WHERE \"Artist\".\"Name\" = 'Alanis Morissette'\n",
            "LIMIT 5;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Handling High-cardinality columns\n",
        "\n",
        "In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\n",
        "\n",
        "One naive strategy it to create a vector store with all the distinct proper nouns that exist in the database. We can then query that vector store each user input and inject the most relevant proper nouns into the prompt."
      ],
      "metadata": {
        "id": "ijiNs1YM4EfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the list of proper nouns from Artist , Album and Genre\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. query_as_list Function\n",
        "\n",
        "Purpose: This function takes a database object (db) and an SQL query (query) as input and returns a cleaned list of strings from the query results.\n",
        "\n",
        "Logic:\n",
        "\n",
        "res = db.run(query): Executes the SQL query against the database and stores the result in res. It is assumed the results are returned as a string representing a list of lists.\n",
        "\n",
        "res = [el for sub in ast.literal_eval(res) for el in sub if el]:\n",
        "\n",
        "ast.literal_eval(res): Safely evaluates the string res as a Python literal.\n",
        "This is used to convert the string representation of a list of lists (from the database) into an actual Python list of lists.\n",
        "\n",
        "[el for sub in ... for el in sub if el]: Flattens the list of lists into a single list. It iterates through each sublist and then through each element (el) in the sublist, adding it to the result if el is not empty.\n",
        "\n",
        "res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]:\n",
        "\n",
        "re.sub(r\"\\b\\d+\\b\", \"\", string): Uses a regular expression to remove standalone numbers (whole numbers) from each string in the list.\n",
        "\n",
        "r\"\\b\\d+\\b\": The regular expression pattern.\n",
        "\n",
        "\\b: Matches a word boundary.\n",
        "\n",
        "\\d+: Matches one or more digits.\n",
        "\n",
        "\\b: Matches another word boundary.\n",
        "\n",
        "\"\": Replaces the matched numbers with an empty string.\n",
        "\n",
        ".strip(): Removes leading and trailing whitespace from each string.\n",
        "\n",
        "return res: Returns the cleaned list of strings.\n",
        "\n",
        "3. Retrieving Proper Nouns\n",
        "\n",
        "This code retrieves proper nouns from three different tables in the database: Artist, Album, and Genre.\n",
        "\n",
        "For each table:\n",
        "\n",
        "It calls the query_as_list function to execute a SELECT query and clean the results.\n",
        "\n",
        "It appends the cleaned results to the proper_nouns list.\n",
        "\n",
        "4. Length and First 5 Elements\n",
        "\n",
        "len(proper_nouns): Calculates and displays the number of proper nouns retrieved.\n",
        "\n",
        "proper_nouns[:5]: Displays the first 5 elements of the proper_nouns list.\n",
        "\n",
        "Summary:\n",
        "\n",
        "This code snippet retrieves data from the database, specifically names of artists, titles of albums, and names of genres. It cleans the data by removing standalone numbers and whitespace, and then combines the results into a single list of proper nouns."
      ],
      "metadata": {
        "id": "RqezEA7d8zl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import re\n",
        "\n",
        "\n",
        "def query_as_list(db, query):\n",
        "    res = db.run(query)\n",
        "    res = [el for sub in ast.literal_eval(res) for el in sub if el]\n",
        "    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]\n",
        "    return res\n",
        "\n",
        "\n",
        "proper_nouns = query_as_list(db, \"SELECT Name FROM Artist\")\n",
        "proper_nouns += query_as_list(db, \"SELECT Title FROM Album\")\n",
        "proper_nouns += query_as_list(db, \"SELECT Name FROM Genre\")\n",
        "len(proper_nouns)\n",
        "proper_nouns[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_4brxYU1vGI",
        "outputId": "94b04c42-7805-4615-c2f4-618dcd52f40e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AC/DC', 'Accept', 'Aerosmith', 'Alanis Morissette', 'Alice In Chains']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(proper_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbft_nlB-Gkf",
        "outputId": "c05aef90-0c71-4c24-ad2d-1df6fb21f73e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "647"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating embeddings from proper noun list and storing in Vector store\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. Creating the Vector Database\n",
        "\n",
        "FAISS.from_texts(...): This is a static method of the FAISS class that creates a FAISS vector database from a list of texts.\n",
        "\n",
        "proper_nouns: This is the list of proper nouns (artist names, album titles, genre names) that we retrieved and cleaned in the previous explanation.\n",
        "\n",
        "OpenAIEmbeddings(): This creates an instance of the OpenAIEmbeddings class, which will be used to generate embeddings for the proper nouns.\n",
        "\n",
        "3. Creating the Retriever\n",
        "\n",
        "vector_db.as_retriever(...): This method creates a retriever object from the FAISS vector database.\n",
        "\n",
        "search_kwargs={\"k\": 15}: This specifies the search parameters for the retriever.\n",
        "\n",
        "k=15: This means that when the retriever is used to search for similar texts, it will return the top 15 most similar results.\n",
        "\n"
      ],
      "metadata": {
        "id": "l2LB0jed-nWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vector_db = FAISS.from_texts(proper_nouns, OpenAIEmbeddings())\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 15})"
      ],
      "metadata": {
        "id": "6iXmfST74Uuq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the final chain\n",
        "\n",
        "1. Import necessary dependencies\n",
        "\n",
        "2. System Message\n",
        "\n",
        "This defines the system message for the LLM.\n",
        "\n",
        "It instructs the LLM to generate syntactically correct SQL queries.\n",
        "\n",
        "It specifies that the LLM should only return the SQL query, without any additional explanations.\n",
        "\n",
        "It includes placeholders for top_k, table_info, and proper_nouns.\n",
        "\n",
        "Key Addition: It now includes a list of proper_nouns that the LLM should use to check the spelling of feature values when filtering. This helps to improve the accuracy of the generated queries by reducing spelling errors.\n",
        "\n",
        "3. Prompt Template\n",
        "\n",
        "This creates a ChatPromptTemplate with the system message and a placeholder for the user's input.\n",
        "\n",
        "4. query_chain Creation\n",
        "\n",
        "This creates a query_chain using the create_sql_query_chain function.\n",
        "\n",
        "llm: Assumed to be an initialized language model.\n",
        "\n",
        "db: Assumed to be an initialized database connection object.\n",
        "\n",
        "prompt: The prompt template created earlier.\n",
        "\n",
        "This chain will take a natural language question and generate an SQL query, execute it, and return the results.\n",
        "\n",
        "5. retriever_chain Creation\n",
        "\n",
        "This creates a retriever_chain that retrieves relevant proper nouns from the vector database.\n",
        "\n",
        "itemgetter(\"question\"): Retrieves the \"question\" from the input dictionary.\n",
        "\n",
        "| retriever: Passes the question to the retriever (created in the previous explanation).\n",
        "\n",
        "| (lambda docs: \"\\n\".join(doc.page_content for doc in docs)): Takes the retrieved documents and concatenates their page_content into a single string, separated by newlines.\n",
        "\n",
        "6. chain Creation\n",
        "\n",
        "This creates the main chain by combining the retriever_chain and the query_chain.\n",
        "\n",
        "RunnablePassthrough.assign(proper_nouns=retriever_chain): This assigns the output of the retriever_chain (the concatenated proper nouns) to the \"proper_nouns\" key in the input dictionary.\n",
        "\n",
        "| query_chain: This passes the updated input dictionary to the query_chain.\n",
        "\n",
        "How it Works (Full Chain)?\n",
        "\n",
        "Input: The chain receives a dictionary with a \"question\" key containing the natural language question.\n",
        "\n",
        "Proper Noun Retrieval: The retriever_chain retrieves relevant proper nouns from the vector database and concatenates them into a single string.\n",
        "\n",
        "Proper Noun Assignment: The RunnablePassthrough assigns the concatenated proper nouns to the \"proper_nouns\" key in the input dictionary.\n",
        "\n",
        "Query Generation and Execution: The query_chain is executed. It uses the \"proper_nouns\" key to know which proper nouns to include in the prompt. It generates an SQL query based on the question and executes it against the database.\n",
        "\n",
        "Result: The chain returns the results of the SQL query."
      ],
      "metadata": {
        "id": "Lav_fL-IAPii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "system = \"\"\"You are a SQLite expert. Given an input question, create a syntactically\n",
        "correct SQLite query to run. Unless otherwise specificed, do not return more than\n",
        "{top_k} rows.\n",
        "\n",
        "Only return the SQL query with no markup or explanation.\n",
        "\n",
        "Here is the relevant table info: {table_info}\n",
        "\n",
        "Here is a non-exhaustive list of possible feature values. If filtering on a feature\n",
        "value make sure to check its spelling against this list first:\n",
        "\n",
        "{proper_nouns}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
        "\n",
        "query_chain = create_sql_query_chain(llm, db, prompt=prompt)\n",
        "retriever_chain = (\n",
        "    itemgetter(\"question\")\n",
        "    | retriever\n",
        "    | (lambda docs: \"\\n\".join(doc.page_content for doc in docs))\n",
        ")\n",
        "chain = RunnablePassthrough.assign(proper_nouns=retriever_chain) | query_chain"
      ],
      "metadata": {
        "id": "-TeDnxTt4a3s"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invoke the chain"
      ],
      "metadata": {
        "id": "FY2wsJ-rBarB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With retrieval\n",
        "query = query_chain.invoke(\n",
        "    {\"question\": \"What are all the genres of elenis moriset songs\", \"proper_nouns\": \"\"}\n",
        ")\n",
        "print(query)\n",
        "db.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mdZWmTZx4fu8",
        "outputId": "e261f808-d467-46f8-b907-d70f6bc2e4a7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT DISTINCT g.Name \n",
            "FROM Genre g \n",
            "JOIN Track t ON g.GenreId = t.GenreId \n",
            "JOIN Album a ON t.AlbumId = a.AlbumId \n",
            "JOIN Artist ar ON a.ArtistId = ar.ArtistId \n",
            "WHERE ar.Name = 'Alanis Morissette';\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[('Rock',)]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without retrieval\n",
        "query = chain.invoke({\"question\": \"What are all the genres of elenis moriset songs\"})\n",
        "print(query)\n",
        "db.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4wDu4eGh4ivL",
        "outputId": "9367b758-bfbf-480b-8f4d-9ad8a9364983"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT DISTINCT Genre.Name \n",
            "FROM Genre \n",
            "JOIN Track ON Genre.GenreId = Track.GenreId \n",
            "WHERE Track.Name LIKE '%Alanis Morissette%';\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UR08F19o4xxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}